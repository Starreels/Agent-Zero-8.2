How to run an Agent 0 for free. In this video I will show you how to use Agent 0 with local models and free APIs. Make sure to watch the previous video about how to install Agent 0 and also make sure you have the latest version which right now is 0.6.4. It contains some bug fixes for Olamma so make sure you are up to date. Let's start by opening the initialized by file in the project route. This is where you can select the main chat model for your Agent, the utility model and embedding model for the memory. By default it is set to OpenAI, GPT-4-Omini-S-Chat and text embedding 3 small as embedding. I recommend using OpenAI or Agent 0 because these models are very cheap and high quality. If you want to go fully free or local I will show you how to change this file accordingly. Before we get to local models let me show you how to run Agent 0 for free using one of the free APIs. At grog.com you can go to Developers, Start Building and here I am already signed in but you will have to create your account. If you are free you can just log in using your Google account. In the API key section you can create a new API key and copy that key into the .env file right next to the API key. Grog is a great online service to allow you to run all of these models completely for free including Lama 3.170b which is a model you will most probably not be able to run locally on your machine. The only limitation is the number of requests per minute, requests per day and tokens per minute but still these limits are high enough for Agent 0 to work smoothly. If you hit any errors regarding these limits you can change the settings and initialize the API file here like the rate limit of requests per minute and tokens per minute so that Agent 0 will wait when it gets close to these limits. Now let me show you Grog connection so I will comment this line out with Get Open AI chat and I will uncomment this line with Get Grog chat. The model is specified here we will be using the Lama 3.170b model and I will also comment out the Open AI Get Embedding model and I will replace it with Get Hugging Facing embedding. This model will run locally on my machine it doesn't need any additional installations so now we got completely rid of Open AI API.

 Now I started the run UI.

py. The UI has started and I can talk to my Grog model. Let's say hi and as you can see the model is very fast and it's also much more capable than 8B models. Sometimes you will have to wait a second or two for the response to start generating. Grog has some cues if there is eye traffic but most of the time it is very fast and responsive. By the way Grog is also the fastest inference provider out there. As you can see with Lama 3.170b it generates 252K per second so even if you have the hardware to run the 3.170b model you will get nowhere near this speed. That is because Grog is manufacturing its own hardware, their language processing units and so far these are the fastest out there. With Grog you have the benefit of running good quality models the fastest way possible and also for free. I would definitely recommend that over running your models locally if your main concern is not privacy. Another great service you can use is openrather.ai. Here again you can sign up with your Google account. Here under your profile icon in the keys section you can create an API key and copy it to the .nv file just like with Grog. This time we will be next to the API key openrather line. You can browse your models, filter them by free and select topically models. Here you can see a collection of models that you can run for free using openrather.ai. Let's try this 405 billion primary model. Obviously this will be much slower than Lama 3.170b on Grog but let's see how this will perform. Let's copy the model name, comment this line out and on comment the get openrather.ai.ai. I think I already have the model name here. Let's restart the agent. Let's see here. As you can see it is much slower than 70b on Grog but let's understand it. It seems to work. The quality of the output should be even better than Lama 370b. As you can see it is much slower and there will be some rate limits as well, probably even lower than on Grog. You can select any of their supported models here. You can use Lama 3.8b, you can use a reflection 70b if you want to try. This is how the reflection 70b model actually works with Agent 0. There seems to be much more traffic on this model so it seems to be slower. It seems to have much bigger delay when responding but it seems to work. Now you can even use the brand new reflection model for your Agent 0. These are two ways to run your Agent 0 fast and free. If you insist on running your models completely locally, you can start with Lama. Go to lama.com and download the installer for your platform. The installation is very simple so I will not be going through that process. Once Lama is installed you need to download the model to be able to use it. We can go to Models section on their website and select Lama 3.1 for example. Here you can select a specific version. I will go with the default and copy this command to Lama, run Lama 3.1 and paste it into my terminal. Lama has to be running in the background but if it's not it should be started by this command as well. My model has already been downloaded in the previous test so right now I can start using it in the terminal already. If you don't yet have the model, let me show you this one on my Windows machine. I will open the power here and paste the same. It will first have to download the model which will take a while. Lama will automatically serve an API endpoint. You can see that when you type Lama surf into the terminal. This will show you the URL address of the endpoint and you can put this address to the .env file. In the example of the envy the default address and port is already there so you will probably don't have to change this unless your address is different for some reason. I don't need my terminal anymore. I am back in Visual Studio Code. I have Lama running in the background and I am commented this line get Lama chat and I specify the model name. And I also commented this embedding. Get Lama embedding line so that I can use Lama for both the chat model, the utility model and embedding. For embedding I will use the Nomicambat text model. Again you will have to download this model first so you will have to run Lama run. We can embed text in your terminal before you run it with your agent. When you are running, I should be able to talk to my agent inside Lama. You can see the GPU spinning. And it works. Don't expect miracles from Lama 3.1 ADB right now to get the time. He tries to use the knowledge tool. I have to tell him. Use code to get him back on track. But I will show you even a better small model you can use with Lama. The model is JL2. Again you will have to download the model using Lama run. And restarted in 0. JL2 is a 9 billion parameter model. But it gives better results than Lama 3.1 ADB.

 It will take a second or two now because the model has to be copied to my video memory first. Then the GPU will spin up. Now I will get the response. And with the get time example, JL2 model will use code execution tool. Instead of trying the knowledge tool first. It might be a hit or miss in some scenarios Lama 3.1 might be better than JL2. But overall I like JL2 better. Even when there were bugs in the framework and the contact window was limited to 2000 tokens. So the conversation with the system prompt did not fully fit into that contact window. JL2 was still able to produce a valid output even with limited instructions. I am running these examples on my Mac because on my Windows machine the GPU is not compatible with Lama. That is probably not going to be your problem. Most GPUs are compatible. But still the model needs to fit into your video memory. If it doesn't fit into your video memory, it will be ran on your CPU and that will be very slow. If you notice your model being very slow, maybe try a smaller model that will fit into your VRM.

 Another application you can use with the Agent 0 for local models is Alim Studio at Alim Studio.ai. Here you can download the installer for your operating system. Again the installation is very simple. And once you run Alim Studio, you can select or search for various models here. This is Lama 3.18B. So let's download this one. And I will also download the embedding model, the same NOMIC model, XWID OLAMA.

 Search for that. And I will find the version NOMIC embed text V1.5 GGUF.

 And here in the variants I will scroll down and download the world precision variant 274M. Once both models are downloaded, I can go to the local server tab. I can select the chat model, Lama 3.1 here, and the embedding model, NOMIC embed text. Here. In these examples, I can see the full name of the model, which I am going to copy and paste into my initialized pi file here, where the Get Alim Studio chat line is. And I do the same for embedding. I can find the exact name of the embedding model here in the embedding tab. Copy this and paste it here. So now Agent 0 is set up to use the Lama 3.18B and the NOMIC embed text both running in Alim Studio. Let's run the framework. Check it in the Y. And it seems to work exactly the same Lama 3.18B as in the NOMIC embedding model. One thing I noticed with Alim Studio is that the GPU doesn't seem to be fully utilized. So maybe Lama 3.18B or maybe Olama is better optimized and will result in a faster inference. But I have no proof like that. Even with this tutorial, I had to quick fix a few bugs, especially with the Alim Studio. So I will be uploading a new version of Agent 0 along with this video. So again, make sure you are running the latest version. And if you don't need to run it on one slow, just use one of the free APIs. It is faster and easier. Okay, that's it for today. I hope you find this video useful and I hope you enjoy playing with your Agent 0. See you next time.