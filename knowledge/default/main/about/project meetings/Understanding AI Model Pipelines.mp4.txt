Hello, party. Okay, sorry. So this is recording for the updates I made for the road map. The basic thing or the difference here is fast. We'll need to, I think that was in the previous one. I remember it wasn't as detailed. It just said, whether or not they made it with their eye, it was just basic. It was like create a model by didn't specify how to do it, as well as the research. It didn't specify how to go about it and the different papers we have, and the different techniques we have. So the changes that are made are fast. Once you get through this part, which is the easier part is, like just understanding the agent zero and how it works, setting it up on your system and all of that. Then now comes to the baseline model. I'm already here. And the basic thing I did was given the baseline model, how it's supposed to work is sort of like a pipeline, let's call it a pipeline, in that once you have our data, it's claimed and all of that. How do we fit it to agent zero and how does agent zero process it? What is the output and how finally how do we submit it to the competition? Apart from that, once you get the results, if the lack of prediction accuracy and all of that, how does it go back into the system such that agent zero can review the results and sort of try different techniques so that it knows, okay, for this question I failed, or for that question I failed, why did I fail and so on. So the pipeline is sort of like a perpetual way that you can test different techniques, different models, different tools and it's, I so I watched a few videos on cargo competitions and it's basically how you would, or we're supposed to, or it's the best way to do it. If you have a pipeline where you can always, you can always, what you call it, you can always try out new stuff without having to change the whole architecture, the whole pipeline of the model you're trying. Then it's really helpful, it does save you lots of time. But basically this part, I think I've covered that. So that's already done, or I have a baseline model, which is like a simple version of it. Then once we have a simple version of it and we've confirmed that it works, then the next thing would be going to the advanced version, so I'll show you the simple and advanced version. And maybe even before that, I'll explain the repos, and I'll come back to the, what you call it, the repo. I mean, then I'll come back to the model of the pipelines. So it should be here, yep. So I have two repositories. If I go to get hub and just under my profile, I have two repos, so one is like the AI, mathematics, or limpied. So this is like for the competition itself, this is the repository that we have final, that would be sort of a reference point for any person who wants to see the work we did. So it has a like a rough project structure. You might not use all of this, but having it in place is better than not having it. It's because this would sort of give us a way of simply, it's fast, it's simple. Then it's more dealer, people can easily track it. And then apart from that, we don't need to use it, then it's fine. For those places, we don't know how to use it, we can learn on that. But yeah, this is the basic project structure and all of that is not that long. But since you're also using a different repository, so here should be, I'll need to change this to, oh yeah, this should be our repository. But yeah, since you'll be using agent zero, how should you agent zero's, agent zero's get hub. So since we're using their repository, I don't think there'll be a limitation. Even though it's an agentic framework, we can still use it. I did ask for complexity. And the only thing they specified is that it has to be open source. The models have to be the weights and all of that have to be, to be available. So for agent zero, the main model, we can choose any open model, where it could be anything on LM studio, hugging face, it's not that, it's not fixed to any particular model. And that is also a strength that we have is that we can try out different models, whether it's a fine-tuned math model or it's a general model, like a, let's say, what's like a llama, llama, 3.2, wherever, yeah, we could use like a general model, but depending on each model's performance, if an old switch out the main model and we can always improve it. So that's one key thing that we have in our toolkit. But back to the repository, this would be the main repo. I won't go through it, but as you can see, there is, the main parts are here, so this would be handling, for this part, agent zero code, this ideally would now be how we integrate our, the competition stuff and agent zero, because agent zero, even though we can use it mainly, the, let me, so I was saying here, even though we can use agent zero to do everything, they still, a way of, they still, a way we need to make it integrate. And this was what I was trying to do in the previous competition. It's, I think it's the harder part, because you now have to start the agent, every individual file, especially like the back-end side, then try and edit that, see what changes it, it has on the agent, agent to functionality and all of that. Then this would be just for data, validation would be validating the, once you send in the, what you call it, the response is to, to cargo and receive them back, we need to validate their accuracy. So this would be sort of like, is this like true, or is it correct or not, then this would now be also feeding back to the agent, it would walk sort of as a tool, as you can see validation tool. Then extension modules, these are more or less tools as well, but separated such that instead of having them in the call, I think this was a really good, this was mostly AI, but the structure is in need. So extensions would be where we put all the tools, or any, this could also be models. So that's the reason why I think it was put separately, because you could basically build an entire model from scratch, and that could be a module or a tool. So having it separate would be ideal rather than having all the code in this spot. Then I think for this ones, it just speaks for themselves. That's nothing difficult, developed. Yeah, this is mostly just project setup. Then notebooks, this would be now, if you open a notebook, just label it under either this name plus your name, then I'll know you're working on this or you're working on that. Then for example, if, so for example, if the model development is for a particular tool, then how it under, it would be better to have it under extensions, because that's entirely the tool by if the model is, if you're working on them, or if you're fine tuning the main model, that agent zero will use, then now you can keep it under notebooks. Then validation tests and all of that. Yeah, you can have that there. Then submission, submission will just be the API called to Kaggle and done then tests. He has been acting off docs. So for docs, maybe one thing to point out is that the reason I'm also trying to record more meetings or record audios, get the transcripts so that we don't get lost and can't do the documentation, but because yes, even if we don't win, or if we get a good position, even if it's number 40, then it doesn't matter what position it is. People on Kaggle do review the different types of solutions, and I could even show you like on the group. On the group here, you'll see, I think it's maybe one of this. Let's see. So you can see this was 40 fast solution, or yeah, 40 fast mind glitches. So he got a gold on it. So people do review each of the solutions and sort of try and see, okay, what methodology did they use? Because even though it might not have been the top solution, it might have been an innovative solution. And this could be something that you can always bring up on your CV or at a job interview or even employers might look at this and maybe use it as like props to your application. So having the documentation part, and even if you record it on yourself and don't need to send it to me, just have it somewhere, or I could even show you like how it's working, sort of like open like a notebook type in, what you're doing then just save it somewhere, we can bring it all together at once and have that documentation. But so this would be public once we finish working on it. Then I have a project resources, now this is private, entirely private. This is just for us. And basically any changes I made, I make in terms of resources that basically push them here, you can always read me, just explain any main factors or any main things, might not update it, but just talks of everything, kind of covered, or first it talks of what we did previously, and then also covers a bit of what we're doing now. So if you want to see anything on the previous stuff we did with Arria, then it's all here, ideally. So, yeah, let's see. Oh, yeah. So yeah, so for all previous resources, it's all here. And I think I invited you to collaborate on both. So let me confirm that. Here's my phone. Let me grab my phone. So yeah, the invite is sent, and also let me confirm the other one. Let's see. Yeah, okay. We have both invite a sent. So if we go to here, and so for the current, okay. So for the current competition resources, now this is what I was going to go through, all the articles and papers that I previously mentioned on here on WhatsApp or Discord are all here, or where I was sorry. I'll hear. So if you get any, make sure you send them on the WhatsApp, then you can always push them here. Then, then, the petition describing this or just personal one. So you can always push the personal ones you had here, sort of like. So like this one would be under AR resources. There is an I label that is if you need any context to give an LLAM, then you can always click this and basically paste that into like O1 and you can use it. So have the rules here, the main page as well. So all of this are more or less under AR should be under AR resources. Then for the agent zero read me file, as well as the entire GitHub, like the whole structure of last code, it's all here. So this is the structure. And I think the one with code is let's see which one it is. Yeah, so this has, it's like the entire GitHub, like the entire GitHub, but as a TXT file. So if you need to paste that in, then it's fine. It's also not that much of context is like number about 50,000. So now we have about 200 K context to 128 K. So this would still be able to go into the prompt part, then which are down. So there is the GPU setup and the GitHub are different. The GPU. So this is the main agent zero, but the GPU setup I'll show you. Right is should be under stars. There is a person actually I should I'm currently working with him on a site project. So I just reached out to him. I noticed he applied or he gave it in zero GPU access, which is basically what it means is that agent zero can basically fast use GPU to create images or can use gp acceleration to. Even if it's building its own model, it can basically do that or find units itself. Basically it can do that because all it would have to do would be. Run a script basically what we do if it's a notebook or it's a Python file, then that's what it's going to do, but that's going to use the GPU to offload anything and needs to. So that was a really good fork. So here it's basically just a fork, everything on agent zero is here, but now the. Agent the GPU setup is now from here comprehensive and video doc a setup or no, boom true. I'm not I did reach out to him asking him if the he made an update because the last update was when I was working on the previous competition. That was about three months ago. So he said he hasn't made one yet, but I'll try this out. You can also try it out. Ideally, I'm not sure if this would work on mark, but we'll try see how I'll try see how it goes. Then I'll update you and update him. Then if not, I'll just use the unit species. Then yeah, maybe most likely use unit species for that. But yeah, there it go been going back to here. So the so the repo of is is here and then you made your changes. This image should be under GPU set up to TXT and they read me file for the GPU is here. Another thing under AR resources is it is transcripts of the the YouTube video. So you'd see all the like the transcript and it's all formatted. It's rather yeah, so this can be context as well. Then what else images and videos. So this would be like the any like source of example I made a recording, verse recording. I'd have the audio, the TXT or the transcription as well as the PDFs. If I don't need to search the different versions, just help basically you can give it to AI or just for reviewing it personally. And also any images we can also add them there, but this would be images that fall under sort of the first to draw like a mock thing and I wanted you to see it. Or I did it in a in a meeting or in a video or in a voice message, then I'll basically put it here rather than in other places. Then guides this would be now the overall roadmarks that we have. So I think this must be the updated one. If I'm not wrong, does let's see the baseline model. I start this. Yeah, yeah, yeah. So that's the updated one, agent zero structure. This was let's see. This was I think this was just a personal thing. It's my guide that was following to coming up with the baseline model. But now since now the context is all finished. I was supposed to now show you the baseline model. I put it on the pipeline. So there I used Claude and oh one, but I think the best one that I came up with is. Full feedback. So let's just open all of them. So then I'll be able to notice. So we have Claude one was this one. This was the Claude base model. So basically the thing I was aiming for is having it perpetual such that. So if so if that I ingestion preprocessing then agent zero code. This is where agent zero is running. Then for us what we'd be doing is create the tool. Give it give agent zero access. For memory enhancement, I think this was already done, but we'll need to confirm this. I think it was done after the new updates, but not sure exactly. Then under basic reasoning module, this would basically be if we have like a tool, then this this or a target you get again. This basic reasoning model is just to tell us whether we need to whether the solution is. Wait, how does this work? Okay, so basic reasoning, we go to basic validation of the output. Okay, okay, after this we assume you already have the predictions from the AI or the answers. This now goes to the reasoning module which tells us is it validated, but this this actually this was in the best base model. That's why it's a bit harder to understand. And let's go to. I'm looking for this one, I think. Now that's the code or the yeah, that's the code feedback on the pipeline. Cloud full model. Let's see this one. Ha ha ha, now it makes okay, it is the correct one. Okay, let's check this one full model. And let me open this. Okay, okay, okay, now it makes sense. So cloud based model. So once we get the answers, this would be validated. This would be this could be a script that we write ourselves or it could be now a tool that the agent uses to validate its own responses. So this would now be like accuracy tools. If it's correct that it's. Now we go to the solution ready for submission, then if yes, then this could be a script that could be added as a tool or it could be. Just a script we have. Then if it's not it goes back. And here this would now basically have to send it back to agent zero core to now process the solution again. Then we have a new solution and now that's the. So if it's now improved and yes, then prepare submissions and it give get feedback then. But this is like basic of the basic or this is the most basic as I can make it in that we just have a way to add tools so we can always add any tools here and make sure agent zero can access them. Then this reasoning module is something we can write ourselves which can function. And then we can write the agent zero and partly without then validation as well this can work with agent zero as a tool or not. Then this as well solution can work with or without this is just to have something that since it's simple it's easier to basically write a script and that can we can work on that and it can run without problems. The full pipeline is now here it's a bit more detailed and I'll go through it. And this would be once we have our baseline model and it works then I would go to the full pipeline in that the full pipeline now helps us the basic things would be doing is just maybe adding like a tool or something or. We can adjust different components but this one that they are they are the good thing with it is just is that it's perpetual or can go it can continue endlessly in that so once we go through the entire loop until we get to. Like strategy optimization and model refinement this can always go back to another place then this can always run perpetually so this would allow the agent to always improve itself without us needing to necessarily even tweak much of things and that's maybe one thing on to implement. So and this can always work for the competition that's why I'm also keen on it is that it doesn't necessarily need to be under this competition if it was like a reasoning competition then this would be not mathematical but it would be more reasoning if it was something else so on and so forth. But yeah once you start the process the data this does a need to be under the agent because this is just like a one time step. But for the second part. Yeah yeah we don't need to be processed the data unless necessary we don't necessarily need to have it under the agent spot because it's I'm also looking at this as if you're training this entire pipeline is just a model we're training we don't need to always refine that the training data as long as it was refined once and it was perfected. So here would have problem analysis and representation so this would be sort of like this would be like a let's say like a parent model that can analyze the problem and give us some sort of road mapping that it's almost like chain of thought in that break it down into steps that once you have the steps now we can know. Okay let's go on to. Okay we'll need this tool we need that tool or we need to do this or that so this problem analysis should enable any model because remember we're not working with all one which can. Do a good problem analysis and basically can't tell us which of the tools we need or the agent would need. Would be working with maybe llama or another model and also it might not be a good model it could be an 8 billion model because there the other thing we'll need to look at later down the line is making it run under 12 hours and get good predictions so having it and also resources as well in terms of resources GPU wise and time then we would also have to it wouldn't be such a good model we use here to be let's say average. So this we could fine tune the model here but the model could be fine tuned in that it can be able to analyze problems better and create better sort of chain of thought or tool choosing capability something like that or we could choose models that are really good at that. But then would be part and recognition so for part and recognition this is basically looking at the agent would go into memory look for okay any question similar to this if yes let's see what kind of solution did they go through so in the solution the basically look at the reasoning okay this is the reasoning steps and after they look at the reasoning steps then most likely since it's similar. Then or it's going to choose the similar types of reasoning patterns for this type of questions then once it has that in mind it goes with it as context onto the next step. So into this steps so since it has or it notice the particular tools it used then it's able will have a section here or script that is able to even agent zero can now you can be able to select to given the context it has from prior solutions as well as the question at hand so it's looking at the question and the previous solutions as well as the. The previous so it's going to look at previous question and solution and current question then that's helps us to get the tool or get the helps the agent get the tool to use. Then once the tool selected then we can now go into agent zero core processing so this is where the agent now starts walking on the solution so depending on the tool that was used now this would be. Agent zero would be calling on to the tool and walking almost like relying on the let's assume this also other agents the tools would be working hand in hand with the agent on the solution as it's processing it so this remember agent zero is it's a good frame of it is intelligent so it's it's almost like oh on the way it can question itself it can. It can refuse things it proposed so in this same way the agents processing would be looking at the tool output sort of is this accurate no it's not accurate door is is this correctly used if not it can tell the tool or it can use the tool again to improve then so it's that's why we have to are this almost perpetual in this spot or there was. Walking together. Then on to the next part would be reasoning and problem solving so for this part ideally if all it needs is the tool and it's all knowledge to answer the question then but that's basically finished it's just going to send the answer down the pipeline and that would be done but if the question needs reasoning and problem solving. Then now this would now be a more in depth part we could argue that to some extent most questions need reasoning but let's say if it's a simple question like one plus one is two or one plus one then that's just basically a tool or it can answer it itself for some of them so there is those types of questions but there's also now there more complex questions. So now it's now going to defer to the reasoning problem and reasoning and problems solving so in this part would have so if we have the solution that the previous if we got a solution here would do a proof generation this would be sort of proving your answer so that if the agent created a proof that sounds bogus it can also question itself. And now that would go back to the model and go back down or go forward in the pipeline but here in terms of the reasoning and problem solving the proof generation as well would be like reviewing the proof itself is it sound is the reasoning that was done or the problem solving done is it sound or it's just hallucinated. Then this would go back to the model is can improve on the particular part or the particular reasoning let's say reasoning structure it came up with improve it right needs to or continue with it so once it continues to the next that would be multi layer validation so this. Could be a specific model or it could be a tool that's sort of let's call it what you call it that's fine tuned to sort of validate the response and by validating I mean fast looking at the at the. At the solution looking at the steps that went into the solution because even one I think I've noticed with some of the questions I think those one was watching was astronomy or astrophysics the solutions were all the steps followed were wrong but the final solution was correct or if not the steps would be sort of accurate but the final solution is let's say slightly off so having. The multi layer validation would look at solution and if it's correct that would go to the next part it would also go into solution validation I think this would be sort of like steps looking at the reasoning steps and they're sort of also the proof that you would have in the proof generation it also have sorry in the proof. So if generation the agent can re walk the proof that or let's say it created its own reasoning structure and it went to be proved is it re is it sound then if yes that went to down the pipeline but now if even if the model shows structure that it thought was good if we go into the multi layer validation and the step was wrong even though the agent thought it was right then now in this part of. So if the solution was wrong the solution validation then they should highlight if they if it would highlight the step that was wrong instead of their entire structure was wrong it would help us track where specifically is maybe reasoning's process or the reasoning structure wrong in most cases so having that to be having that as a thing to be track to be really good and I think I think I would be very happy with that. Also so it in outlier they really look for in the reasoning tasks they track where exactly or in which step did the model fail such that if you're now finding the model you know this step or now once you're finding you'd have the step there the solution and maybe like the wrong solution the wrong structure. The wrong reasoning structure as well as the wrong the particular step that was wrong so that model will learn in this type of structure this was wrong and it was wrong because of this step then this would go back into the validation part then then this could also now be stored or we can always track this separately but now if the solution is valid including the step if let's let's also disregard the step for now since it's now saved here or the reasoning if there's an issue then now or not then we can already part of that but now if the answer is correct would go into if it was correct given the reasoning structure that we had then would go into saving the pattern and the pattern just basically means the answer is correct. So if the reasoning pattern that it had then is solution then if the solution is wrong then now we need to improve and if we need to improve this could now we'd have multiple strategies and the strategies would be does the solution need improvements so yes if yes would go to a different solution. So it could now it could prompt the agent to improve the reasoning and in this part would also have the step remember would also have the step where it was wrong as well as this the particular structure that was made earlier that went down the line as well as the proof so would have the proof of the structure why the reason or why the agent thought this was a good structure to go through then would also have the step that was wrong as well as the particular question and the intended answer or yep and oh yes since you also have the questions the training questions that we have this would now be would also have the answer yeah would have the question and answer so proof reasoning structure step that was wrong. And question and answer then here would basically adjust the reasoning with all that context so that would be one strategy so let's say if we go through this step where we took all of that improve the reasoning a couple times using that just strategy one but the model really fails to adjust the reasoning to a good enough extent where it's now correct what would do we do. So the tool is try a different strategy but the main thing would aim for is maybe having just maybe adjust the reasoning then that could work for most questions but for maybe fewer questions would have to try different tools and in this part maybe would need something like Montecarlo tree search or a different technique it doesn't matter what technique is used. But you'd have a different tool that can be used then let's say for for the for the excuse me so for any solutions that also go through the pipeline they fail in the reasoning or the model can't do the reasoning part as well as the tools then what would do we can always add another strategy to the model. So this could be instead of adjust reasoning the way we have it here would have strategy to would be let's say Montecarlo tree search strategy three would be let's say self refinement or any other method then we can always add tools here more or more less endlessly till we get a solution to that particular question. And it's also it's almost like probing but the good thing here with the reasoning where it we're also making something intelligent and this is also working remember we're not talking with an old one model will be working with the 8 billion model even though it's fine tuned for math it might not be fine tuned for reasoning all it knows is just answer this way once at that way so having this spot here would be. A strong thing on our end then once you have a correct solution then the pattern is stored so this would be in the pattern storage as well would also be storing the improvements reasoning wise as well as the tool why or the tool tool use here such that this can always go into the next part. So the pattern stored always goes into memory and the method how would have the pattern so they should be reasoning as well as how the reasoning was adjusted if it was adjusted then tool performance or tool use so how was the tool used so here would have the agent sort of as it saves start. So if you have a graph in memory would have to create it's going to create take itself how was this tool called was it called appropriately so if it was a calculator for example did I do two times two correctly did I do two divided by two then gave an answer for two times two because that's wrong tool use. Apart from that to also add solution history so this would always the this core memory system helps us to track this main parts and now this would always come back to. Here pattern recognition every time it gets a new question because remember the once this is perfect it once our pipelines perfected and sent into cargo what they basically do they have a private test set that we don't see so that private test set is now new question that the model hasn't seen yet so now the agent seeing a new question it won't be like it's going to it's not going to hallucinate it's going to check for. Partons faster then any question similar to this what was the reasoning step what was the like where he did fail in terms of two performance where did a particular reasoning structure fail so it's going to look through this this could also be. We could have a but that's under here so this would be a script but here the agent is in charge of its own memory similar to how it charge it is in charge of it's the memory it's all. But onto the next part the next part would be so if you get a correct solution would go to prepare submission then for this part I can notice that. Is the solution is correct but doesn't need improvement then now would still prepare the submission because the solution was correct would prepare the submission then submit it to cargo then process then once we get the feedback from cargo via API they should basically go through back to the agent and it's going to review each question. Sort of like if I did a test and the teacher goes through it so agent the agent will also go through the solutions itself and see okay where did I oh okay how did I perform where my time need to improve on so would I need to improve memory or I need to improve two performance anywhere so this would have been maybe two performance here as a call. To or to the bin tool performance in terms of like something like a proof part here but mainly here let's say mainly here then how it could also be here in terms of strategy wise so this could also be like one of the main features where once it gets the newer questions it can. Review the feedback in let's say almost in real time or it can review the feedback once it gets it then it can update the relevant parts accordingly so maybe in terms of the tool maybe the tool is might have passed all other steps but here we still need to be improved slightly could even be like a formatting issue or something. But then if not if yes then this would go into updating it updating it update the memory here updating the tool specifically and this would be the back end performance of it if now improvement is needed then the pipelines finished. By in terms of two performance analysis it's going to look at also I think in this part should also be looking at the code itself but that could be complex let's just say how the tool performed in general because remember we'll be coding this out or if it's a model that we build or find you would have done that so we might not be the best we may not have done. Just this to it in terms of how it works even though it might work half as good so here once you analyze how the true performs strategy optimization this would basically could even be as simple as the agent telling us okay this is the improvements you need to make in this manner and this is why then we can go and do that or we could also ask it to do it because it's the easy way to do it. It's still our access to the file so it could go out and improve the strategy itself and this most likely I think it's referring to this part the particular tool here so ideally this could also help in improving the strategies we use here then once we get once this is improved to a reasonable level or yeah that's what we're going to do. The next part would be model refinement and model refinement would be for me I'm looking at this whole structure as the model refinement so by model refinement I think they just mean improve the relevant part where it needs to be improved then that's finished then after that once it's refined this goes into pattern recognition hard as that walk. Trying to understand this model refinement okay I get you I get you so if the model is doing this by itself if it does two performance analysis by itself and strategy optimization by itself it's going to refine it's more the model it by itself as well and also once it's improved it's now going to prompt sort of like a nap date in the pattern recognition part where it's able to notice that a tool was updated so so for example if we went through this whole process and the strategy all strategies failed as well then now this would be like last results so two performance strategy refine then now try and do the solution again that doesn't work. We try it again and again so it's going to be our last layer of protection in terms of trying to probe through and solve the particular problem but basically that's it and I think it's a it might need my some slight improvement but I think I specially after doing this recording I think it is really air out even some issues that are hard with it you it did look a bit more don't think because it's big and detailed and lots of things going on but after reviewing it now it all makes sense and I think it's really solid has really good the good things about is that we could use the same structure different competition this could also be the structure we use for our past our custom AI assistant for example so this could be a good thing for us to do is really good. We are way of us to sort of have perpetual coding like a perpetual code assistant or something so this could always turn out to be a product that could be sold so just having it here and making sure we walk on it is not the next part so I think the next thing would be now starting basically the pipeline itself. The base one I don't want to start with it but from the previous competition the one thing I think one thing I learned was that it's good to have your base model because with this one with the full pipeline anything could go wrong here even though I've explained it here and it sounds like heaven everything could go wrong. So having the base model something that we can track by ourselves we can always we can always know where exactly to improve and make the improvement and also it's perpetual in that now will be the ones having to make the improvement rather than the agent doing the improvement. Then let's say let's say what comes to us to let's say a month out of or let's say a few weeks to the final deadline we can always end the previous structure didn't work we can still use this as our main as our main fallback for that. But I think last thing I think basically that's it let's see here yeah basically that's it I've gone through the resource repo I've gone through the other repo gone through what's in them so you can see everything that's there I think here oh lastly would be this. And this is just like I labeled it this way because it's just the competition itself now if you need anything to do with the competition specifically in terms of in terms of let that open up or yeah. So all the resources provided like the test PSV how they provided on the connection so so on the repo itself then referring to it online if need be I think it I don't think there is anything else let me do. Here the tuition yeah let's see here yeah I can see data no box okay the previous with ours that's fine look any final updates. So. Okay so I think maybe one last thing is. Let me let that send it out once it was done or this is another benchmark that I wish was interesting I think it was one of the benchmarks that has shown how. Like even one almost AR models failed on it or performed poorly on it in that if it's fast it's a math benchmark but also not sure why the internet is slow. And it's not good to hear. Pause this. Pause. Pause. Pause. I think it's back but yeah we got the frontier math so the main thing it does it's really difficult we can not hear. Then it was tested on all one for. Lord Sonnet new and old and then they all failed in that they performed really low and I think we they claim that it's one of the benchmarks that still holds true and shows that AI models are still. Poor at math or poor. To some great extent but this also took time if it was done by humans and top tier humans is still took time so it's. Still good. It's still it's still hard either way if it's for humans or AI doesn't really matter but another thing. I wanted is that they do have a few like questions and solutions so maybe having like this type of questions even if they're even we could use all one to create. Like a small data set out of this then we can use that to sort of like once the final the full final pipeline ready we can use this one to sort of probe the model into more complex thinking and more complex improvements because. Assuming this questions are smarter or way harder than the international map of limpia then this would be a good way to sort of have the model see what's hard or what would be complicated and what would be the process of more complicated questions because we never know maybe. The particular private set that they give us might be harder than the previous training training versions that we used. So this is the paper it's all on what's up and I think I already added in the project resources but yeah maybe let me see the performance. Oh yeah they did interview some mathematicians like Terence how if you know him he's a sealed medalist but yeah he did say it's like a hard it's really hard even for humans and he's one of the best he just said they're really hard so we can see here's the performance like yeah it's no any maybe they should have put like human. To see what's the comparison to like humans but it's okay but yeah I basically finished with the recording if you have any questions feel free to ask or if you have or if you want me to go through this again and. I'm going to start data side because that's the fasting the pipeline so what I'll do is I'll go through. I'll start the date we have their prior date by look for data I did find someone called or not called but hugging face for particular benchmark as well as this one so hard that's ready and cleaned and all of that then I'll miss laughing good to the next time if you want to start then it's fine but yeah I think that's the best. That's basically it.