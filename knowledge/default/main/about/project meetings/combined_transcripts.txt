We split our data to 7000 that we have into half. 

So first, in the first half, we just keep trying the same shit that you said like first you give it a question and then you get the answer. 

The reasoning plus the answer and then you reuse the reasoning for the other things. 

For the one it used. For the next answers. 

For the next questions. Okay. First of the 50. 

For the first 50. I do get it. Sort of like train and test. 

Train would be the fast 50 do it yourself. 

Yeah and then the next ones only use the ones that are available. 

Only the ones that are available. Yeah. The one that I came up with in the first half. Or the answers. Yeah. 

It got correct. Yes. And the reasoning only. There's no need for the answers along as long as we have a reasoning. 

You can generalize them. Yeah. Rather than make them question specific. The reasonings. 

Because ideally, I was thinking for the second part. You see that fast 50 did it alone. The second 50 now we give it its solution. 

Such that it can go back, check where in the previous ones maybe what did I go wrong. 

Then it's able to critique that. Then as well as in this part. It's able to learn different patterns since it has a solution. Reasoning and the answer. 

So it's sort of like hitting two goals. Okay. But that will most likely have to call that ourselves. Yeah. Yeah. Yeah. Sorry. We can try that. Okay. 

So that would be the reasoning tool. Split 50, 50. Fast 50 does itself track the reasoning under logs. 

And solutions see which ones are correct, which ones are not. Then in the next 50 we give it the solution and the reasoning behind it. 

Then half the system or the tool critique the previous 50 or the fast 50, which are which reasoning steps are correct. Then after that for the second 50 track the particular patterns that were followed and save that in memory. Okay. 

The second half instead of coming up with new ones, you would specifically try to improve the ones that have already stored. Yep. Specifically try to do that. While in the first half we will do both. Like in case there isn't one that's already been made then we'll create one. 

But if there is one that can be improved and it will improve that one. Yep. Okay. Maybe the question can be the best reasoning also. Question in mind is though we'll figure that out. Could it be a tool or a model because it could either be a script that we that it can run. 

Then basically that's how it does it or it could be a fine tune model that does that. But since it's base model maybe we could just go for a basic tool. Then that's finished. I don't know. I'll watch the videos on agents yours too. Yeah. I don't know what the tools are. Then for the accuracy tool. 

Accuracy tool I think that just checks if it's correct. But now since if this was to go to the cargo submission it would ignore if it's correct. Maybe here in instead of accuracy half it has checks pattern recognition. Yep. So such that it can search in memory for the pattern stored. 

Then then if it even if it's sent to to cargo you can check any patterns that was stored. Recognize any then use that to solve the question that it's given at that current time. Maybe it hasn't seen any of that kind. Then that would be the validation. But yeah I think that's okay. 

Okay so first step finish the data thing then do the tools. Yep. Come off with the possible tools to find them like specifically. Here? And then try to build them. No that's these types. Not here. Yeah. First we've got to define them properly before we get into them. Just like a normal project we've got to make the SRS first. 

Just to be exactly what we're working on. You can be changed later on but you can have something to work with. Like a plan. Good plan. Not just theoretical but like in terms of the tools in share. For the other one. For this one it's the goal was make it simple but still yeah we'll need the plan.

Okay, speak your pre-log. You're in serious. This is a recording for the competition AI, but the Magical Olympiad. Progress too. So like a pre-face, we tried the previous competition, but didn't go too well. We finally bailed the model, but nah nah. That didn't operate on the first competition. Against the rub and not. Let's go deeper then. What is this competition about? It's basically the same shit, right? They just want us... I'll show you. The previous start from the previous one, since I have it here. Let's go with this one. Hey, Barbie. The previous top solution, I think, was this one. Hopefully. No meaner. Yeah, this was the top in the previous one. And what they did was this. They did just a pre-face. They have a natural reasoning like data. So this was done, I think, with GPT-4O. Come up with an answer, sort of like pseudo code. Then code nested and tool interaction data. So this would be... So for example, if you have an agent, it will have some tools. So for this part, they created data to show how the tools would be used. And how it interacts with the pseudo answer. Then from there, now we give it the question. The question goes into a chain of thoughts. Then the code... So the process would be... From the chain of thoughts, we get code. From the code, outputs. From the outputs. If there's a narrow-witty bug, go back to the code and so on. If it's successful, that's our final answer. And that's basically it. Then that's more and more. Yeah. You can't see it. You have to speak. But the main thing they did was our let me go back. Self-consistency with tool integration reasoning. Then chain of thoughts as well. That was one thing. And if I'm not wrong, there must have been some sort of tree of thought. So those are tree of thought. Or actually just let's say, yeah. So we had... We had input, output prompting. Then chain of thoughts. That's what we've covered. Self-consistency. Chain of thought. Then tree of thought. So that was a few of the techniques covered. Then that's it. Then results. That was the top model. At least that won the previous one. And now the scores that it had were... I think 29 over 50. Maybe here. Now that's the demo. Go back to the communication. Oh yeah. Yeah, 29. So on the private set, the test... The private test that we don't see, they go 29 out of 50 questions. So they usually give human students 50 questions. Then I think humans have surpassed... Maybe I'm not sure about... About 30 plus. But for Numeena, the scores 29, which are the best. As we continue now, the competition we're trying out. The target is... I don't think there is a target. But let's see. Is it the highest or at least 47? Yep. So the target is 47. Out of 50. Out of 50, okay. Then... I'm going 5 million. No, no. I don't know. High score? I just saw it down there. That's total. Yes, the high score. Okay. Okay. Sorry? Yeah? The overall price, progress price, winner in this competition. The price will be at least billionth. Okay. If no team, blah blah blah. Just that. We'll try to hit there, but we'll likely land somewhere. I just want cargo for this operator. I don't want no gun. They know winning now. We'll end up somewhere hopefully. Somewhere... Yeah, I need to chill. We've got five months. Good boy. Seven days ago, that's when he started. But that's all the preface we need. So... So what I did is... The technique I have in mind... I'm not going to show you Agent Zero. But the technique I have in mind... It's not that we don't need to use it. It's just something I want to try out. Let me know such for it. Toby be good. Agent Zero. There we go. So Agent Zero is a framework that uses... It's an agent framework that uses an LLM as its core... Let's say core... Yeah, backbone, perfect. With the... It's sort of like a cursor composer how you can prompt it. Then it goes... And does or creates a file. Add the code in and so on and so forth. But it's more general purpose. So it can... If you use cases, if you give it GPU access, it can... If you prompt it to do something with GPU acceleration, then it can do that. It can retain thoughts or prompts or... Anything really. In its memory, but the memory is more efficient compared to what we have on GBT. The memory is more of a learning capability in that... In its process, we can tell it to learn by itself or keep store things in memory. Like best, let's say, best techniques or best results, something like that. Apart from that, it does have a doc container where that's where it does most of its code things. So that it's separate from your computer. So in case it does... Let's say, was case scenario, it deletes windows or deletes linux. Then it doesn't affect your system. It only happens on the doc container. Then it does... When we say it is a computer or computer as a tool, then with the doc container, it can't interact with your computer. Almost similar to the anthropic one, but not as such. The anthropic one is different because it's directly on your computer. Rather than this, it's through the doc container. Next, multi-agent cooperation. This is an interesting one. So with multi-agent, it means it can create different iterations of itself. So we could have... The reason it was called agent zero, agent zero, be at the top. Or let's say, no, I'm... I'm agent infinity before zero. Then agent zero is now the agent itself. Then it can delegate tasks to agent one. Agent one can give to two or agent zero can give to two and so on. And that basically helps. If I was to tell it, go create a website, for example, that uses Node.js as backend and maybe react as front and something like that. Then that would mean there's a react agent, there's a Node.js agent, then there's maybe... Now agent zero is there, overlooking person. Then as it overlooks, agent one repost to agent zero, agent zero can send agent one back, go redo this, and so on and so forth. You can always interrupt any of the agents, but it all happens within the same terminal. I'll probably show a video. The next we have customizable. So one of the reasons I chose this technique, and I tried it for the ACK price. The ACK price was looking to sort of solve puzzles that humans can easily solve just by looking at them. But agent, I chose agent zero because it's customizable in that you can bring in anything. If it's Monte Carlo tree thought, you can bring in the system. Either as an AI model, independent from it, and they collaborate or you could give it as a tool. So it depends on how you structure the tool. Then it's extensible in that... But it's more or less scalable. It just depends on how you manage the code. I think that's where I really couldn't get for. Because there's multiple files, etc. Communication is the main thing. All of the agents running. It's just done through prompt files. So that's going to be a big thing. It's also a good thing in that there's less... It's a competition and we need certain staff to be done. Then we'll have to code. But most of the things will happen through the prompt. Because that's what guides the AI system through different steps. So let's say fast prompt is create our website. Then it would fast go to maybe web search check for best techniques. Then there is a prompt for web searching. If it's tool use, there is prompts for tool use, etc. There could be dangerous. It's not pre-programmed. If you cannot provide the ideal environment, let your agent know. So it's more or less just telling us to be careful. Because it is using our computer. It's not using someone else's. So you can still do stuff on it. If you take it that far. Another cool thing is that the agent can't be autonomous to some certain extent. Let's say if I let it run in a task, it can go and do. If it goes across errors, it can solve them. Basically just use it the LLM. It has in its main logic it's able to do the reasoning. Let's say the reasoning we see from O1. But it's now doing it with whichever model you want. Lastly set up. Did I see GPU? Then you can put GPU access. But yeah that's it. Now next I've got to show you the video of the founder. But this is just a technique. We don't need to use this. I just think it's cool. It's an agent so it's forward looking. The good thing also, we just need to give it tools. We already have, it's like having like us work separately. Then basically just give us tools. So our tools could be the way we use Claude on the side, GPT on the side, maybe sometimes CASA or every time CASA. So it's just the more powerful the tools, the better the agent. So now I think we only have web search. To use, to use could be calculator and what else? Could code tool, think of some, I think there is a memory tool to store stuff in memory. Those are the ones I can remember. Yeah. Then let me ask Dion to do the videos fast. Show the guy or like more ideas. I can watch the videos today. Go with the next idea. This one was, so just let me open the actual link. I think it's someone, oh yeah, there we go. Now not this. This I want there. Let me start it. They want, they want us geometry. Also, you see, now geometry, yep. And hopefully they are done somewhere here. Is it this one? There we go. That's one. And the second one. Here, there. Here. I think it's same. No, no, no, separate. So one of the cool things we saw before, oh, one released. That's when you went to Luton. It was, deep mind released, alpha geometry. Actually, that was early in the year, but maybe I don't know. But alpha geometry, what it did, it got to 25. 25, those was this score on international master limp. It was like more or less highest score for an AI. If I'm not wrong, it was closer to gold. We are projecting it to be someone in the future, whatever they are projecting. But the techniques they used are more or less interesting. So this was mainly for geometric stuff. So like in the previous one, remember how we had to not collect images, questions with images, ETC, because it would be hard for the AI to answer, because it can't maybe see the plot or decide for the plot, ETC. So alpha geometry would now you have your problem, goes to an LLAM. With the LLAM, we have the symbolic engine. I'll have to maybe go through down here. Then now makes the solution. But more or less, the symbolic engine, if I'm not wrong, let's see. Let's just go there. Wait, is it the same thing? It's just fucking same thing. Okay, it's finished. But for my blemages, let's just go through this. Okay, okay. I don't want to say the DSL, I really don't want to say that. Okay, I think I got what they did. The symbolic engine basically from here, from here, why is it? From here, it basically sort of describes how the plot or image comes about. So you'd have like a cyclic, let's say sort of like a phantahedron of sorts. Then you'd give it the angle, angle EAH equals to angle EDH, something like that. Then you have a series of steps that comes up with the image represented as words. Then that goes into the LLAM. Then using that knowledge, we're able to reason through and more or less construct. This is more or less in the back, and at least I don't think we can see that. Maybe they could, yeah, they could, how the LLAM would construct the image using the information we gave it. So this was one of the techniques I thought would be interesting. To get maybe 47 out of 50, that means more or less. We have to also capture like images, most likely, it was 47 out of 50, basically the whole test. Oh, more or less the whole test. No, the test, the reason for your hard-to-do images. Really? There's images at all. Look at the date of the test. No, no, leave the test. The test one is, it tells you which one is similar. But it's all from here, remember? No, but tells you exactly what it's not included. Okay, yeah. Check. I think, I think that was about the previous one. I'm not sure about this one. No, 100 was I'm sure. The previous one, that's what it said. Check on date maybe. Oh, it's already in this chat. It's all already in this chat. I asked it. No, but it's in the chat. It's all in there. I'm going to be a face today on that. Yeah, yeah. Let's go up to the first one. I'm going to go up to the first message. And add it to that one. Yeah, just add it to this one. You added 100 or so. No. You guys all, yeah. Yeah, go back up there. No, no, this is too long. Yeah. Oh. Yeah. Oh, yeah. You can do something. Oh, I have to wait for the streets. I'm going to go home after this. Okay. You can do it. It's all over. I'm eating the open mind. Okay. Even for geometric problems, no diagrams will be provided. The geometric question is. That means that all of the questions. Yeah. There will only have the necessary details for the drawings in question. Okay. But not diagrams. Yeah, there will not have enough details for you to actually create the image. Depends. Depends. If it's because some questions, some questions you'd get them like the ones here. They won't allow you to do that. It gives you like the angle. You see like this question. That basically gives you the plot. No, no. Even if you don't see it. You can just construct the way they did. That's why they brought in the idea. You won't give me the full picture though. Yeah, through the words. As we can, how do you man can? No. And this is how they did it. That's it when they have the image itself. Okay, I have an image to start with. And they turn that into words. And then they use that to give the AI sort of image through the words. But if we have this, we can try and create this. We don't need to, we already have the words itself. Okay. Oh yeah. They don't have the words that we don't have the image. But they have the questions. It's in AI more. They give this to humans. They give it this and this. Yep. So this and this. This and yeah, we only have this. But in... We're more than able to create that. Yeah. I will see. You just can try. You can put it there. Mm-hmm. I think that's one that was... No! This one. Oh, she totally did this one. Oh, hell no. Just return it then. No, this is why the monitor is off. She'll go on. How is it there? Oh, no. The monitor's there. How is it that long? She's. It can't be that long. It's... She'll long? She'll go on. See, see, see, four. Is it on? I think it's on. I think it's on. I think it's on. What's up, you can turn it on. Found the second one. Found the third one. Yeah, this one. It's still from the point. This one, yeah, this one was one you're in the middle. It achieved silVER, but this was I AMOVE. I AMOVE. Let's see, but you know, there's results. Closer to... Closer to what you call it, this thing. Clown T visas. All for proof was more of reasoning behind the... Now, the previous step was the one we went through. So you get your problem, goes to formalize or basically uses a let's see solve a network. Oh my God, is it even here? Let's see. Is there a pay calling? Is it here? Yeah, there is. You know actually. It did perform well. It's down model. I think from what I remember in this part, it just goes into like a DSL, the DSL deconstructs the question, comes up. Now the DSL is like a code language, a specific language that if for example if it's our agent we give it a tool that it can create a DSL to decompose March questions into a specific type of code, maybe let's say something like, let's say, Mamed for example, Mamed makes some types of charts. So if our DSL is Mamed, the formalize a network does that decompose the question, gives us the how you'd explain it in that language, then that becomes your formal problems. Then you use that to train the solve a network, then the solve a network. If I'm not sure now we do search, yeah, from there I think I'm lost. But yeah, from there, let's say formal proofs, we search through solve a network, we give and what are we giving it, the question in that code. It tries to solve the question, but now we take out the proofs. So for example, let me say formal proofs, I think what they meant by trying to get the paper itself, you try and check, let's say if ask GPT1 plus 1 is 2 and give me a reason why, we take out the reason then that's our formal proof. Then now we use that, if the formal proof was wrong, if I said 1 plus 1 is 2 because 0 plus 1 is 2 then that's our wrong reason, you should be saying another good reason. So we take the good reasons of the formal proofs. Sometimes I think they also used human formal proofs, sort of adding the reasoning component, then use that to train the solve a network. Now they improved, the final improved version of the solve a network is now the alpha proof. Now that's what they, sorry, now that's the final model that's perfect, but alpha proof is the whole system. So just to go through quickly, if you have a maths problem, take it through a DS, take it through Gemini itself, Gemini converts it to a particular DSL, DSL it doesn't matter, it just needs to be a language that can easily communicate maths, maths concepts and language and formalities, then that becomes our formal problems, we use the formal problems to train our solve a network, that's the fast round of training, then from there we ask Gemini again to solve the questions and give formal proofs, we take out the formal proofs that are correct, then we use that plus human added formal proofs or improved formal proof versions, use that to train the solve a network the second time, then that gives us our final model. The reason I pointed this out is because of this part, the DSL, that's really interesting because that's also another way in our prize, people used it to decompose the image, for example, if you see an image that's in the fast chart, one square moved up, then left, then right, then we know most likely the next part, for humans it's easy, the next part would be back left, since we started from top went right, went down, so most likely it's left, so for humans it's easy but for AI it's hard because you can't see, so the best or some methods that were used was DSL, explain it, explain the image, fast what is, how does the image look like, now that's how the DSL helps us because it's able to describe it in a language AI, can understand, it could be binary, it could be whatever the language, then maybe the formal proofs part where we collect that, use that in, so far it could be used that in memory, maybe memory, we could choose to train a model but that's later on, think after that there is nothing, agent zero, the videos, review, previous solutions, checks auto methods, okay, let's see, this is just to have it there, then we'll do it once we officially start, but let's see any other ideas, this are the main ones, okay, there is a few more, this are the videos, okay, let me not start with tasks yet, so with agent zero I think I haven't watched the latest video but from what I left off they didn't have a good memory layer, so we might have to get either an open source version, use that as a memory layer, the memory layer would be sort of like a network, maybe neural network, if I'm not wrong, that is optimized for memory and retrieval and I think for this one we have graph memory specifics, I don't go into specifics but it's called memory AI, when we get there we'll see how it goes or I think they might have implemented it but this was suggested by the founder or the creator, apart from that there was another one, another memory layer, so this would be foundation memory layer for agents and the reason we need the memory layer is because the learning component will become hard because it's like if I go to GPT ask it to do this, it does it and forgets, even if we tell it save it in memory that's like just memory that goes into the memory we have here, most likely if we tell in our next prompt say use them, for example as you said go and use the information you have in memory, now that's put in the kv cache or it's put in like somewhere in the system before the prompt is sent in, so you have your memory plus context plus I think create let's say that, if you attach documents then that also, then there is drag but now all that goes into the LLAM gives you answer, so the reason we want this is because it helps us manage how the AI would be able to fast identify the good things about itself, then second to save it, it can always recall them by itself, so it might implement more complicated drag but we'll see how it goes, the more memory we have but hopefully this should help us if, yeah but we'll see one of the truth either way, I think this one's paid, it's free, there's open source, so either most likely we'd have to go with the open source version because with competition we can't really use closed source stuff, I think that's what we would, free bar, yeah, look at the difference, yeah but the problem is like it's closed source, so most likely if you wanted to see what they or how they come up with this, like there you see how we would check GitHub for this, like this, now we can see all the code but let's maybe check the GitHub, but tonight we'll see, don't want to go to deep into that, but yeah, we'll see, then that's competition, okay another idea would be since we have O1 now, create reasoning data set, so that would mean send it a question, tell it to answer, check the answer if it's correct, then now maybe tell it to create the data set from that point or check the actual reasoning steps that it took, maybe this may be clean enough, but now the problem is this are only summarized, the ones we see are only summarized versions of the full, you call it full reasoning steps, so maybe you'd have to prompt it to be a bit more detailed, even if they have restricted it, we might have to push it out from the system, ask it to give us more details on the such reasoning to make the data set, then this would go into a reasoning tool depending on how the tool structure would look like, then the reasoning tool would be sort of maybe a precursor to every question, so once it gets a question from the competition goes through the reasoning tool or user the reasoning tool, user that, user that, from that that goes to context for the LLAM, then plus the answer pretty much, then that can be submitted to cargo, then this adjust previous solutions, I'm going to that, this adjust to refile box, see the techniques they used, then I think that's the last, oh yeah, extra resources, this one is LLAM reasoning, research papers, yeah this adjust extra resources, when you get to that point you'll basically come back to this, find the resources that are helpful, use them in the process, then open it, I'm proving mathematical reasoning, yeah that should be a good one, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, I'm going to introduce this, yeah paper introduces reason evaluation and the machine evaluate mathematical reasoning beyond accuracy, yeah but this adjusts, let me just say this adjusts now the extra concepts that will go through, once I did create sort of like a workflow, might not be the best might not be the best, but I just told on to create it, so week one would be wait, let me check, let me check here, this would be like most of the weight is actually done, so create feedback that needs to be done, review previous top solutions, so I put the links here, so that's basically done, then we'll use them down the line, that's captured in the workflow, check state of the art methods in using other lamps for math including Google DeepMinds or Copenhagen Ice Model, I'll need to add in all one stuff, but maybe that's in the, or this part, the open and high part maybe, then find articles trending or all that have interesting techniques, so maybe the thing will have to take note is with Numina, let me actually open it, bug up, I think it was this one, here, yeah, with Numina, when they did this part of the chain of thought and this was at the time this was more state, let me say state of the art, or it was like something trending, so you'd sit on LinkedIn, those are the papers are you sending, so these are things you'd see on LinkedIn, then people are trying them in the competition, send the feedback and so on and so forth, so usually you find some of the top winning methods have already used like the latest thing like Monte Carlo Treesert or something of the sort, so like tool integration would be something you'd have now and you have agents, but this was already back then when agents was in Tivanna thing, so it's basically trying to see, okay what's new, can it, is it helpful, if it is, then let's implement and improve, then I think, yeah, use notebook LM for reading articles, this was just like a point, if you want, so you can create podcasts for you based on the articles and it's fairly cool, then when we start we'll need to check the data we collected, see why it lacks, EEG need to collect data from questions with images, not sure, so I'm not exactly sure how we collected it, I've kind of forgot, maybe, yeah, we'll have to check if there's sort of like questions with images or like a plot or something like an angle thing, then we'd have to maybe collect more or connect images, yeah, so we might have to look for some, yeah, most likely, even might be better off if we do the separate time, if we collect the second time, then we'd have separate data set with just tags, then separate with just images, then you can treat them separately in the process, then now the workflow, yeah, that's the second last thing, so week one and two, depending on when we start, would be just checking through agent zero, understanding, checking the videos, framework, the GitHub, basically understanding how the system works, try to view the code, maybe trick the code, see how it's, I've done most of that, this was basically for you and actually me, just I didn't do most of the coding part, even though I was working on it a lot, so I'll have to go through it again, then from the research and prioritization, that would be identify the techniques, so some we might have reviewed them, some might be might have seen on LinkedIn, from maybe someone told you HEC, then just read them some arise relevant papers, then discuss, so the maybe one thing I noticed from the previous competition, I was trying to implement everything and that's, it's not a good thing because it's now complex, you have too many files to handle, you have too many complex techniques, so that was the best choice that I made, but for this, it would be better, if we discuss a technique, maybe let's say today it's Monte Carlo research, then we take a week or two, try and implement with this, splitting it could be halfway, like you can do research, I can do research or some person does research, I can, or you do research, I do code, vice versa, it depends, you'll have to discuss that, then implementation and experimentation, so it would have cycles of walks or like this would be walk week one to two, that cycle one, then second, then now the second cycle, HEC, so it would have design control, implemented selected techniques, so this would be sort of, let's say even if it's like rough, we go to draw IO or any software, or even just or no one or any LLM, from the summaries we made, techniques we chose, see how it would actually walk theoretically rather than implementing it fast, then seeing okay this doesn't walk, so that would be design control, if you want to go in depth, that would be maybe try coding something, something to present, that doesn't need to be perfect, then from there we implement the selected technique of the above, then test in the value, this would now include sending the submission to cargo, getting the results back, reviewing that, seeing how we need to improve, maybe even identifying where the steps lock, whether it's the reasoning part or the CCC, okay then now we go to evaluation, yeah I think that's covered, these are yeah findings, then scaling and integration, this would be like once, so this part would be iteration, that would be like a cycle, whichever techniques we tried, maybe we tried Monte Carlo and chain of thought this time, cycle two will try, let's say using a DSL and maybe Monte Carlo was fairly good, so we tried those two, then if Monte Carlo is still good, the DSL isn't as good, would maybe put it here, take maybe a different technique, try that, so depending on the good, the best performing techniques would retain them, then simply attach other methods, another way could be also trying different combinations, but now complexity comes in, so we'll see, I think that's why it takes five months, but now the details, let's see what I didn't capture, this would just be an up, this would be something at the open on the day, so it's actually just only to be detailed, debug step through, modify program, okay this is just guiding us on what to do there, okay establish baseline model, so that's from week three, so it depends if you want we could build, or even let's, let me not even use the fucking wild build, let's just say fine tune, fine tune is way easier than build from scratch, so it would take like, even a modell that's already fine tuned for maths, then fine tune it with, let's say the question data, or reasoning data, or ETC, then that could be our custom reasoning model, then that could be a tool, so the agent has access to another tool, that's an actual model, then say say same shade it could be, if it's not reasoning it could be DSL part and ETC, just depends, you could either choose the key things would be thinking of is how to implement a technique as a model trained or fine tuned, or even rag, or is it implemented as a tool, and the tool would most likely be coded, hard coded, or open source, yeah, then keep the model simple, to isolate effects, so with the models, yeah keep them simple, I think fine tuning would be best of, I'll add that in, how do you write this, fine tune, is a big element, raise the type thing, it can't type it's a pocket figure, sometimes you can, yeah, oh there is, yeah, here, here this way, type, that way, fine tune, okay, but I'll have to save it, but yeah, next initial submission, but that's for here it's assuming the agent is a model, but it's more of a framework, or another thing maybe it's not captured here, we could have the agent submit the results by itself, but it's not necessary, we just need the results, then that goes, we can submit that ourselves, then document the base, yeah, documentation is it as important, but it's just good practice such that even if we are number 200, 300, if someone just wants to go through, maybe it was highly voted, it's good to have something to guide someone with, so like this, just something you can even if it's rough or even casually written, then it's okay, then week four research prioritizes, so that would be the research week, yeah, basically go through all techniques, use the best summarise, we could even speak with MSK, I did put like a, so every four weeks around this point would basically go to any mentor, or any even Dr. Mu, ask them anything could be specific to the research, or it could be specific to other parts, just to enhance sort of like information you can get from other people without having to source it ourselves, because it's not easier to, you might find MSK can tell us in touch a minute rather than searching through the whole week, or even Dr. Mu can tell us something that would take us longer, then depending on the technique we choose, implement, test, then submit, then it's just a cycle, implement, test, submit, then analyse, then if there's improvements, we can do like a mini improvement on that part, if that works in this part, if we analyse and improve on now this system, then could be a loop there, or more or less it's the best techniques within the ones that were tested and analysed that could be combined or even kept separate, so that's, it's more like even versioning, the way it do versioning and get the best, the best version is saved, then you can have, let's say we have best COT and best MCTS, then those could be two good models, try combining, that's your third, see how those work, then continue, then I think, so for some of the techniques I think O1 chose to put them like self-consistency, it wouldn't be smart to put it while we are still choosing a method, because self-consistency is more like helping the model improve on its current self, so if you have a trained model and wanted to improve on how it operates, you'd add self-consistency methods with chain of thought ETC, so this would be, it was smart to put it here, and I think, yeah, integrating external tools, what's this, or this would be simpy, simpy, I knew it, simpy, not simpy, no, it came out, this could be a DSL, it's used to do some math, but yeah, all of this is basically symbolic mathematical representation, then it's sort of, if you wanted to do like modulus, and there's a sign for modulus in human language, you can get it here somehow, then it's all explained, so that could be the method used for the DSL, modify agent architecture, this was placed wrongly, this is basically, while we're testing this, if we choose to go with the main route is the agent, then that would be, would be testing and implementing via the agent, but if we choose to build a model like a mega model from scratch or fine tuning, then that would be, would be improving and like looping through the model itself rather than the agent, just dependent the main technique which is, then text, test complex problem solving, that's for the agent, basically I think this is, because we don't know how the agent does on new information, that's why this is here, then most likely that would be solved with the memory, so it's able to recall, maybe that question had an interesting technique, such that we can remember the technique used, then submission, yeah, I'm down, synthetic, yeah, it would, if we build a model, this would come early, if we choose to go the agent route, this would come in later, once we have a method chosen, then now would use the synthetic data and fine tuning to improve on those techniques, so we'd find a way to plug in this into the, so this could be a custom model added as a tool, or, if not, it could be a technique, let's just say technique one, technique two, technique three, agent can choose technique one, two or three, or it could, in the reasoning process, make sure you go through all techniques, which can also be saved in memory, so this system will have to think through that, then week 10 to 12, iteration and scaling, scaling not as such depends, because that's still early, that's how many three months, 12, yeah, three months, that's three quarters of the way, let's say iteration, so this would be iteration, let's say we have a sub-good model, let's say it even scored 15, for example, then iteration would be seeing where can we improve on optimizing more libraries, more methods, curriculum, combined techniques, reinforcement learning, so this part is miscellaneous, most likely, by the end, because it's not as important, ongoing processes would be documentation, even documentation could be, let's say as you read the article, you could open a recording, just record the process, or if you're coding, record the process, or even at the end, or at the start, record the few thoughts that you had on what you're trying to implement, then regular meetings doesn't need to be weekly, could be, the circles would be one to two weeks, so maybe one, if you want it weekly, maybe two weeks, depends with also schedules, but let's just say weekly for now, then experiment tracking, or weights and biases, will we need it, or if we build a model from scratch, but we'll be fine tuning, yeah, if we choose to build a model, then even if it's for the agent, we can use this adjust experimental, weights and biases, just helps you track the losses, and the learning rate, ETC, log-or-l-experiments, parameters, results, this, I think, this would also be good, since what are, the work I was doing on the previous competition helped me learn all about Docker, because every day I had to log in to Docker, set up the Docker run, the agent, so if we can try and implement techniques industry standard, then this will gonna help us some time down the line, then it's in central repository, okay, so we'll need to make a GitHub and a free camp, I think there is one, there was one from the previous one, I'll just have to change it up, so then additional resources, just links, Trello is more like project management, Asana, if I'm not wrong, most likely it is, okay, like workflow management, okay, so we'll see which is the best, then version, yeah, data, yeah, keep raw process, yeah, collaboration slack, most likely it's better to use slack, because slack, if I could show you, slack is just like a, is it like a discord, let me just open on free, it's just more professional, keeps us separate from WhatsApp, because sometimes we might open WhatsApp, see something weird, then just become totally distracted, but this is basically how it looks, you have different channels, if you open a channel, you have like things here, then you can upload files, then could be specific, it's just connect different stuff, but it's basically that, so the free, no, no, it's free, but the pro version just gives you like a longer history, it's not necessary, even us we don't use it, it depends if you want them, because when I joined I didn't need them, unless now you're like CEO for, yeah, we'll only be able to access last three months of our work, yeah, but is it, that means we're only putting four months for, we won't be able to access the first months of our work, it's not necessary, we could use discord, yeah, this is creating the ass, trying to ensing, the upgrade to the page, oh, I missed it, I'm seeing too much, this is gonna be like for the whole group, yeah, 17, $1,000, just, thank you much, oh, I lied, then final time management, continuous learning, because it's cool team, and how management is gonna be tough? let's just say if we don't do something every day, we got to go gym, man, we just, how many days are we gonna put in depends? one, one each week? yeah, that's what I can do at best, right now, yes, maybe during winter I can do like four days, yeah, but when does it end? it doesn't, five months is like February March, March most likely, yeah, someone there, yeah, that's fine, hmm, good shit, then for week, for cold, like, managing different tasks, choose the one you like, we don't need this, oh, here I also put the specific cases, most likely, most of the time, we can use our own framework, why isn't, so I get two stuff into all these, no, no, no, this one, I think that's mentoring game, yeah, that's remote, keep it there, okay, I gotta speak with the cow, don't that yet? I ain't gonna speak to him for Bob, yeah, that's it, good, desi, good, I need time to proceed now,

Hello, party. Okay, sorry. So this is recording for the updates I made for the road map. The basic thing or the difference here is fast. We'll need to, I think that was in the previous one. I remember it wasn't as detailed. It just said, whether or not they made it with their eye, it was just basic. It was like create a model by didn't specify how to do it, as well as the research. It didn't specify how to go about it and the different papers we have, and the different techniques we have. So the changes that are made are fast. Once you get through this part, which is the easier part is, like just understanding the agent zero and how it works, setting it up on your system and all of that. Then now comes to the baseline model. I'm already here. And the basic thing I did was given the baseline model, how it's supposed to work is sort of like a pipeline, let's call it a pipeline, in that once you have our data, it's claimed and all of that. How do we fit it to agent zero and how does agent zero process it? What is the output and how finally how do we submit it to the competition? Apart from that, once you get the results, if the lack of prediction accuracy and all of that, how does it go back into the system such that agent zero can review the results and sort of try different techniques so that it knows, okay, for this question I failed, or for that question I failed, why did I fail and so on. So the pipeline is sort of like a perpetual way that you can test different techniques, different models, different tools and it's, I so I watched a few videos on cargo competitions and it's basically how you would, or we're supposed to, or it's the best way to do it. If you have a pipeline where you can always, you can always, what you call it, you can always try out new stuff without having to change the whole architecture, the whole pipeline of the model you're trying. Then it's really helpful, it does save you lots of time. But basically this part, I think I've covered that. So that's already done, or I have a baseline model, which is like a simple version of it. Then once we have a simple version of it and we've confirmed that it works, then the next thing would be going to the advanced version, so I'll show you the simple and advanced version. And maybe even before that, I'll explain the repos, and I'll come back to the, what you call it, the repo. I mean, then I'll come back to the model of the pipelines. So it should be here, yep. So I have two repositories. If I go to get hub and just under my profile, I have two repos, so one is like the AI, mathematics, or limpied. So this is like for the competition itself, this is the repository that we have final, that would be sort of a reference point for any person who wants to see the work we did. So it has a like a rough project structure. You might not use all of this, but having it in place is better than not having it. It's because this would sort of give us a way of simply, it's fast, it's simple. Then it's more dealer, people can easily track it. And then apart from that, we don't need to use it, then it's fine. For those places, we don't know how to use it, we can learn on that. But yeah, this is the basic project structure and all of that is not that long. But since you're also using a different repository, so here should be, I'll need to change this to, oh yeah, this should be our repository. But yeah, since you'll be using agent zero, how should you agent zero's, agent zero's get hub. So since we're using their repository, I don't think there'll be a limitation. Even though it's an agentic framework, we can still use it. I did ask for complexity. And the only thing they specified is that it has to be open source. The models have to be the weights and all of that have to be, to be available. So for agent zero, the main model, we can choose any open model, where it could be anything on LM studio, hugging face, it's not that, it's not fixed to any particular model. And that is also a strength that we have is that we can try out different models, whether it's a fine-tuned math model or it's a general model, like a, let's say, what's like a llama, llama, 3.2, wherever, yeah, we could use like a general model, but depending on each model's performance, if an old switch out the main model and we can always improve it. So that's one key thing that we have in our toolkit. But back to the repository, this would be the main repo. I won't go through it, but as you can see, there is, the main parts are here, so this would be handling, for this part, agent zero code, this ideally would now be how we integrate our, the competition stuff and agent zero, because agent zero, even though we can use it mainly, the, let me, so I was saying here, even though we can use agent zero to do everything, they still, a way of, they still, a way we need to make it integrate. And this was what I was trying to do in the previous competition. It's, I think it's the harder part, because you now have to start the agent, every individual file, especially like the back-end side, then try and edit that, see what changes it, it has on the agent, agent to functionality and all of that. Then this would be just for data, validation would be validating the, once you send in the, what you call it, the response is to, to cargo and receive them back, we need to validate their accuracy. So this would be sort of like, is this like true, or is it correct or not, then this would now be also feeding back to the agent, it would walk sort of as a tool, as you can see validation tool. Then extension modules, these are more or less tools as well, but separated such that instead of having them in the call, I think this was a really good, this was mostly AI, but the structure is in need. So extensions would be where we put all the tools, or any, this could also be models. So that's the reason why I think it was put separately, because you could basically build an entire model from scratch, and that could be a module or a tool. So having it separate would be ideal rather than having all the code in this spot. Then I think for this ones, it just speaks for themselves. That's nothing difficult, developed. Yeah, this is mostly just project setup. Then notebooks, this would be now, if you open a notebook, just label it under either this name plus your name, then I'll know you're working on this or you're working on that. Then for example, if, so for example, if the model development is for a particular tool, then how it under, it would be better to have it under extensions, because that's entirely the tool by if the model is, if you're working on them, or if you're fine tuning the main model, that agent zero will use, then now you can keep it under notebooks. Then validation tests and all of that. Yeah, you can have that there. Then submission, submission will just be the API called to Kaggle and done then tests. He has been acting off docs. So for docs, maybe one thing to point out is that the reason I'm also trying to record more meetings or record audios, get the transcripts so that we don't get lost and can't do the documentation, but because yes, even if we don't win, or if we get a good position, even if it's number 40, then it doesn't matter what position it is. People on Kaggle do review the different types of solutions, and I could even show you like on the group. On the group here, you'll see, I think it's maybe one of this. Let's see. So you can see this was 40 fast solution, or yeah, 40 fast mind glitches. So he got a gold on it. So people do review each of the solutions and sort of try and see, okay, what methodology did they use? Because even though it might not have been the top solution, it might have been an innovative solution. And this could be something that you can always bring up on your CV or at a job interview or even employers might look at this and maybe use it as like props to your application. So having the documentation part, and even if you record it on yourself and don't need to send it to me, just have it somewhere, or I could even show you like how it's working, sort of like open like a notebook type in, what you're doing then just save it somewhere, we can bring it all together at once and have that documentation. But so this would be public once we finish working on it. Then I have a project resources, now this is private, entirely private. This is just for us. And basically any changes I made, I make in terms of resources that basically push them here, you can always read me, just explain any main factors or any main things, might not update it, but just talks of everything, kind of covered, or first it talks of what we did previously, and then also covers a bit of what we're doing now. So if you want to see anything on the previous stuff we did with Arria, then it's all here, ideally. So, yeah, let's see. Oh, yeah. So yeah, so for all previous resources, it's all here. And I think I invited you to collaborate on both. So let me confirm that. Here's my phone. Let me grab my phone. So yeah, the invite is sent, and also let me confirm the other one. Let's see. Yeah, okay. We have both invite a sent. So if we go to here, and so for the current, okay. So for the current competition resources, now this is what I was going to go through, all the articles and papers that I previously mentioned on here on WhatsApp or Discord are all here, or where I was sorry. I'll hear. So if you get any, make sure you send them on the WhatsApp, then you can always push them here. Then, then, the petition describing this or just personal one. So you can always push the personal ones you had here, sort of like. So like this one would be under AR resources. There is an I label that is if you need any context to give an LLAM, then you can always click this and basically paste that into like O1 and you can use it. So have the rules here, the main page as well. So all of this are more or less under AR should be under AR resources. Then for the agent zero read me file, as well as the entire GitHub, like the whole structure of last code, it's all here. So this is the structure. And I think the one with code is let's see which one it is. Yeah, so this has, it's like the entire GitHub, like the entire GitHub, but as a TXT file. So if you need to paste that in, then it's fine. It's also not that much of context is like number about 50,000. So now we have about 200 K context to 128 K. So this would still be able to go into the prompt part, then which are down. So there is the GPU setup and the GitHub are different. The GPU. So this is the main agent zero, but the GPU setup I'll show you. Right is should be under stars. There is a person actually I should I'm currently working with him on a site project. So I just reached out to him. I noticed he applied or he gave it in zero GPU access, which is basically what it means is that agent zero can basically fast use GPU to create images or can use gp acceleration to. Even if it's building its own model, it can basically do that or find units itself. Basically it can do that because all it would have to do would be. Run a script basically what we do if it's a notebook or it's a Python file, then that's what it's going to do, but that's going to use the GPU to offload anything and needs to. So that was a really good fork. So here it's basically just a fork, everything on agent zero is here, but now the. Agent the GPU setup is now from here comprehensive and video doc a setup or no, boom true. I'm not I did reach out to him asking him if the he made an update because the last update was when I was working on the previous competition. That was about three months ago. So he said he hasn't made one yet, but I'll try this out. You can also try it out. Ideally, I'm not sure if this would work on mark, but we'll try see how I'll try see how it goes. Then I'll update you and update him. Then if not, I'll just use the unit species. Then yeah, maybe most likely use unit species for that. But yeah, there it go been going back to here. So the so the repo of is is here and then you made your changes. This image should be under GPU set up to TXT and they read me file for the GPU is here. Another thing under AR resources is it is transcripts of the the YouTube video. So you'd see all the like the transcript and it's all formatted. It's rather yeah, so this can be context as well. Then what else images and videos. So this would be like the any like source of example I made a recording, verse recording. I'd have the audio, the TXT or the transcription as well as the PDFs. If I don't need to search the different versions, just help basically you can give it to AI or just for reviewing it personally. And also any images we can also add them there, but this would be images that fall under sort of the first to draw like a mock thing and I wanted you to see it. Or I did it in a in a meeting or in a video or in a voice message, then I'll basically put it here rather than in other places. Then guides this would be now the overall roadmarks that we have. So I think this must be the updated one. If I'm not wrong, does let's see the baseline model. I start this. Yeah, yeah, yeah. So that's the updated one, agent zero structure. This was let's see. This was I think this was just a personal thing. It's my guide that was following to coming up with the baseline model. But now since now the context is all finished. I was supposed to now show you the baseline model. I put it on the pipeline. So there I used Claude and oh one, but I think the best one that I came up with is. Full feedback. So let's just open all of them. So then I'll be able to notice. So we have Claude one was this one. This was the Claude base model. So basically the thing I was aiming for is having it perpetual such that. So if so if that I ingestion preprocessing then agent zero code. This is where agent zero is running. Then for us what we'd be doing is create the tool. Give it give agent zero access. For memory enhancement, I think this was already done, but we'll need to confirm this. I think it was done after the new updates, but not sure exactly. Then under basic reasoning module, this would basically be if we have like a tool, then this this or a target you get again. This basic reasoning model is just to tell us whether we need to whether the solution is. Wait, how does this work? Okay, so basic reasoning, we go to basic validation of the output. Okay, okay, after this we assume you already have the predictions from the AI or the answers. This now goes to the reasoning module which tells us is it validated, but this this actually this was in the best base model. That's why it's a bit harder to understand. And let's go to. I'm looking for this one, I think. Now that's the code or the yeah, that's the code feedback on the pipeline. Cloud full model. Let's see this one. Ha ha ha, now it makes okay, it is the correct one. Okay, let's check this one full model. And let me open this. Okay, okay, okay, now it makes sense. So cloud based model. So once we get the answers, this would be validated. This would be this could be a script that we write ourselves or it could be now a tool that the agent uses to validate its own responses. So this would now be like accuracy tools. If it's correct that it's. Now we go to the solution ready for submission, then if yes, then this could be a script that could be added as a tool or it could be. Just a script we have. Then if it's not it goes back. And here this would now basically have to send it back to agent zero core to now process the solution again. Then we have a new solution and now that's the. So if it's now improved and yes, then prepare submissions and it give get feedback then. But this is like basic of the basic or this is the most basic as I can make it in that we just have a way to add tools so we can always add any tools here and make sure agent zero can access them. Then this reasoning module is something we can write ourselves which can function. And then we can write the agent zero and partly without then validation as well this can work with agent zero as a tool or not. Then this as well solution can work with or without this is just to have something that since it's simple it's easier to basically write a script and that can we can work on that and it can run without problems. The full pipeline is now here it's a bit more detailed and I'll go through it. And this would be once we have our baseline model and it works then I would go to the full pipeline in that the full pipeline now helps us the basic things would be doing is just maybe adding like a tool or something or. We can adjust different components but this one that they are they are the good thing with it is just is that it's perpetual or can go it can continue endlessly in that so once we go through the entire loop until we get to. Like strategy optimization and model refinement this can always go back to another place then this can always run perpetually so this would allow the agent to always improve itself without us needing to necessarily even tweak much of things and that's maybe one thing on to implement. So and this can always work for the competition that's why I'm also keen on it is that it doesn't necessarily need to be under this competition if it was like a reasoning competition then this would be not mathematical but it would be more reasoning if it was something else so on and so forth. But yeah once you start the process the data this does a need to be under the agent because this is just like a one time step. But for the second part. Yeah yeah we don't need to be processed the data unless necessary we don't necessarily need to have it under the agent spot because it's I'm also looking at this as if you're training this entire pipeline is just a model we're training we don't need to always refine that the training data as long as it was refined once and it was perfected. So here would have problem analysis and representation so this would be sort of like this would be like a let's say like a parent model that can analyze the problem and give us some sort of road mapping that it's almost like chain of thought in that break it down into steps that once you have the steps now we can know. Okay let's go on to. Okay we'll need this tool we need that tool or we need to do this or that so this problem analysis should enable any model because remember we're not working with all one which can. Do a good problem analysis and basically can't tell us which of the tools we need or the agent would need. Would be working with maybe llama or another model and also it might not be a good model it could be an 8 billion model because there the other thing we'll need to look at later down the line is making it run under 12 hours and get good predictions so having it and also resources as well in terms of resources GPU wise and time then we would also have to it wouldn't be such a good model we use here to be let's say average. So this we could fine tune the model here but the model could be fine tuned in that it can be able to analyze problems better and create better sort of chain of thought or tool choosing capability something like that or we could choose models that are really good at that. But then would be part and recognition so for part and recognition this is basically looking at the agent would go into memory look for okay any question similar to this if yes let's see what kind of solution did they go through so in the solution the basically look at the reasoning okay this is the reasoning steps and after they look at the reasoning steps then most likely since it's similar. Then or it's going to choose the similar types of reasoning patterns for this type of questions then once it has that in mind it goes with it as context onto the next step. So into this steps so since it has or it notice the particular tools it used then it's able will have a section here or script that is able to even agent zero can now you can be able to select to given the context it has from prior solutions as well as the question at hand so it's looking at the question and the previous solutions as well as the. The previous so it's going to look at previous question and solution and current question then that's helps us to get the tool or get the helps the agent get the tool to use. Then once the tool selected then we can now go into agent zero core processing so this is where the agent now starts walking on the solution so depending on the tool that was used now this would be. Agent zero would be calling on to the tool and walking almost like relying on the let's assume this also other agents the tools would be working hand in hand with the agent on the solution as it's processing it so this remember agent zero is it's a good frame of it is intelligent so it's it's almost like oh on the way it can question itself it can. It can refuse things it proposed so in this same way the agents processing would be looking at the tool output sort of is this accurate no it's not accurate door is is this correctly used if not it can tell the tool or it can use the tool again to improve then so it's that's why we have to are this almost perpetual in this spot or there was. Walking together. Then on to the next part would be reasoning and problem solving so for this part ideally if all it needs is the tool and it's all knowledge to answer the question then but that's basically finished it's just going to send the answer down the pipeline and that would be done but if the question needs reasoning and problem solving. Then now this would now be a more in depth part we could argue that to some extent most questions need reasoning but let's say if it's a simple question like one plus one is two or one plus one then that's just basically a tool or it can answer it itself for some of them so there is those types of questions but there's also now there more complex questions. So now it's now going to defer to the reasoning problem and reasoning and problems solving so in this part would have so if we have the solution that the previous if we got a solution here would do a proof generation this would be sort of proving your answer so that if the agent created a proof that sounds bogus it can also question itself. And now that would go back to the model and go back down or go forward in the pipeline but here in terms of the reasoning and problem solving the proof generation as well would be like reviewing the proof itself is it sound is the reasoning that was done or the problem solving done is it sound or it's just hallucinated. Then this would go back to the model is can improve on the particular part or the particular reasoning let's say reasoning structure it came up with improve it right needs to or continue with it so once it continues to the next that would be multi layer validation so this. Could be a specific model or it could be a tool that's sort of let's call it what you call it that's fine tuned to sort of validate the response and by validating I mean fast looking at the at the. At the solution looking at the steps that went into the solution because even one I think I've noticed with some of the questions I think those one was watching was astronomy or astrophysics the solutions were all the steps followed were wrong but the final solution was correct or if not the steps would be sort of accurate but the final solution is let's say slightly off so having. The multi layer validation would look at solution and if it's correct that would go to the next part it would also go into solution validation I think this would be sort of like steps looking at the reasoning steps and they're sort of also the proof that you would have in the proof generation it also have sorry in the proof. So if generation the agent can re walk the proof that or let's say it created its own reasoning structure and it went to be proved is it re is it sound then if yes that went to down the pipeline but now if even if the model shows structure that it thought was good if we go into the multi layer validation and the step was wrong even though the agent thought it was right then now in this part of. So if the solution was wrong the solution validation then they should highlight if they if it would highlight the step that was wrong instead of their entire structure was wrong it would help us track where specifically is maybe reasoning's process or the reasoning structure wrong in most cases so having that to be having that as a thing to be track to be really good and I think I think I would be very happy with that. Also so it in outlier they really look for in the reasoning tasks they track where exactly or in which step did the model fail such that if you're now finding the model you know this step or now once you're finding you'd have the step there the solution and maybe like the wrong solution the wrong structure. The wrong reasoning structure as well as the wrong the particular step that was wrong so that model will learn in this type of structure this was wrong and it was wrong because of this step then this would go back into the validation part then then this could also now be stored or we can always track this separately but now if the solution is valid including the step if let's let's also disregard the step for now since it's now saved here or the reasoning if there's an issue then now or not then we can already part of that but now if the answer is correct would go into if it was correct given the reasoning structure that we had then would go into saving the pattern and the pattern just basically means the answer is correct. So if the reasoning pattern that it had then is solution then if the solution is wrong then now we need to improve and if we need to improve this could now we'd have multiple strategies and the strategies would be does the solution need improvements so yes if yes would go to a different solution. So it could now it could prompt the agent to improve the reasoning and in this part would also have the step remember would also have the step where it was wrong as well as this the particular structure that was made earlier that went down the line as well as the proof so would have the proof of the structure why the reason or why the agent thought this was a good structure to go through then would also have the step that was wrong as well as the particular question and the intended answer or yep and oh yes since you also have the questions the training questions that we have this would now be would also have the answer yeah would have the question and answer so proof reasoning structure step that was wrong. And question and answer then here would basically adjust the reasoning with all that context so that would be one strategy so let's say if we go through this step where we took all of that improve the reasoning a couple times using that just strategy one but the model really fails to adjust the reasoning to a good enough extent where it's now correct what would do we do. So the tool is try a different strategy but the main thing would aim for is maybe having just maybe adjust the reasoning then that could work for most questions but for maybe fewer questions would have to try different tools and in this part maybe would need something like Montecarlo tree search or a different technique it doesn't matter what technique is used. But you'd have a different tool that can be used then let's say for for the for the excuse me so for any solutions that also go through the pipeline they fail in the reasoning or the model can't do the reasoning part as well as the tools then what would do we can always add another strategy to the model. So this could be instead of adjust reasoning the way we have it here would have strategy to would be let's say Montecarlo tree search strategy three would be let's say self refinement or any other method then we can always add tools here more or more less endlessly till we get a solution to that particular question. And it's also it's almost like probing but the good thing here with the reasoning where it we're also making something intelligent and this is also working remember we're not talking with an old one model will be working with the 8 billion model even though it's fine tuned for math it might not be fine tuned for reasoning all it knows is just answer this way once at that way so having this spot here would be. A strong thing on our end then once you have a correct solution then the pattern is stored so this would be in the pattern storage as well would also be storing the improvements reasoning wise as well as the tool why or the tool tool use here such that this can always go into the next part. So the pattern stored always goes into memory and the method how would have the pattern so they should be reasoning as well as how the reasoning was adjusted if it was adjusted then tool performance or tool use so how was the tool used so here would have the agent sort of as it saves start. So if you have a graph in memory would have to create it's going to create take itself how was this tool called was it called appropriately so if it was a calculator for example did I do two times two correctly did I do two divided by two then gave an answer for two times two because that's wrong tool use. Apart from that to also add solution history so this would always the this core memory system helps us to track this main parts and now this would always come back to. Here pattern recognition every time it gets a new question because remember the once this is perfect it once our pipelines perfected and sent into cargo what they basically do they have a private test set that we don't see so that private test set is now new question that the model hasn't seen yet so now the agent seeing a new question it won't be like it's going to it's not going to hallucinate it's going to check for. Partons faster then any question similar to this what was the reasoning step what was the like where he did fail in terms of two performance where did a particular reasoning structure fail so it's going to look through this this could also be. We could have a but that's under here so this would be a script but here the agent is in charge of its own memory similar to how it charge it is in charge of it's the memory it's all. But onto the next part the next part would be so if you get a correct solution would go to prepare submission then for this part I can notice that. Is the solution is correct but doesn't need improvement then now would still prepare the submission because the solution was correct would prepare the submission then submit it to cargo then process then once we get the feedback from cargo via API they should basically go through back to the agent and it's going to review each question. Sort of like if I did a test and the teacher goes through it so agent the agent will also go through the solutions itself and see okay where did I oh okay how did I perform where my time need to improve on so would I need to improve memory or I need to improve two performance anywhere so this would have been maybe two performance here as a call. To or to the bin tool performance in terms of like something like a proof part here but mainly here let's say mainly here then how it could also be here in terms of strategy wise so this could also be like one of the main features where once it gets the newer questions it can. Review the feedback in let's say almost in real time or it can review the feedback once it gets it then it can update the relevant parts accordingly so maybe in terms of the tool maybe the tool is might have passed all other steps but here we still need to be improved slightly could even be like a formatting issue or something. But then if not if yes then this would go into updating it updating it update the memory here updating the tool specifically and this would be the back end performance of it if now improvement is needed then the pipelines finished. By in terms of two performance analysis it's going to look at also I think in this part should also be looking at the code itself but that could be complex let's just say how the tool performed in general because remember we'll be coding this out or if it's a model that we build or find you would have done that so we might not be the best we may not have done. Just this to it in terms of how it works even though it might work half as good so here once you analyze how the true performs strategy optimization this would basically could even be as simple as the agent telling us okay this is the improvements you need to make in this manner and this is why then we can go and do that or we could also ask it to do it because it's the easy way to do it. It's still our access to the file so it could go out and improve the strategy itself and this most likely I think it's referring to this part the particular tool here so ideally this could also help in improving the strategies we use here then once we get once this is improved to a reasonable level or yeah that's what we're going to do. The next part would be model refinement and model refinement would be for me I'm looking at this whole structure as the model refinement so by model refinement I think they just mean improve the relevant part where it needs to be improved then that's finished then after that once it's refined this goes into pattern recognition hard as that walk. Trying to understand this model refinement okay I get you I get you so if the model is doing this by itself if it does two performance analysis by itself and strategy optimization by itself it's going to refine it's more the model it by itself as well and also once it's improved it's now going to prompt sort of like a nap date in the pattern recognition part where it's able to notice that a tool was updated so so for example if we went through this whole process and the strategy all strategies failed as well then now this would be like last results so two performance strategy refine then now try and do the solution again that doesn't work. We try it again and again so it's going to be our last layer of protection in terms of trying to probe through and solve the particular problem but basically that's it and I think it's a it might need my some slight improvement but I think I specially after doing this recording I think it is really air out even some issues that are hard with it you it did look a bit more don't think because it's big and detailed and lots of things going on but after reviewing it now it all makes sense and I think it's really solid has really good the good things about is that we could use the same structure different competition this could also be the structure we use for our past our custom AI assistant for example so this could be a good thing for us to do is really good. We are way of us to sort of have perpetual coding like a perpetual code assistant or something so this could always turn out to be a product that could be sold so just having it here and making sure we walk on it is not the next part so I think the next thing would be now starting basically the pipeline itself. The base one I don't want to start with it but from the previous competition the one thing I think one thing I learned was that it's good to have your base model because with this one with the full pipeline anything could go wrong here even though I've explained it here and it sounds like heaven everything could go wrong. So having the base model something that we can track by ourselves we can always we can always know where exactly to improve and make the improvement and also it's perpetual in that now will be the ones having to make the improvement rather than the agent doing the improvement. Then let's say let's say what comes to us to let's say a month out of or let's say a few weeks to the final deadline we can always end the previous structure didn't work we can still use this as our main as our main fallback for that. But I think last thing I think basically that's it let's see here yeah basically that's it I've gone through the resource repo I've gone through the other repo gone through what's in them so you can see everything that's there I think here oh lastly would be this. And this is just like I labeled it this way because it's just the competition itself now if you need anything to do with the competition specifically in terms of in terms of let that open up or yeah. So all the resources provided like the test PSV how they provided on the connection so so on the repo itself then referring to it online if need be I think it I don't think there is anything else let me do. Here the tuition yeah let's see here yeah I can see data no box okay the previous with ours that's fine look any final updates. So. Okay so I think maybe one last thing is. Let me let that send it out once it was done or this is another benchmark that I wish was interesting I think it was one of the benchmarks that has shown how. Like even one almost AR models failed on it or performed poorly on it in that if it's fast it's a math benchmark but also not sure why the internet is slow. And it's not good to hear. Pause this. Pause. Pause. Pause. I think it's back but yeah we got the frontier math so the main thing it does it's really difficult we can not hear. Then it was tested on all one for. Lord Sonnet new and old and then they all failed in that they performed really low and I think we they claim that it's one of the benchmarks that still holds true and shows that AI models are still. Poor at math or poor. To some great extent but this also took time if it was done by humans and top tier humans is still took time so it's. Still good. It's still it's still hard either way if it's for humans or AI doesn't really matter but another thing. I wanted is that they do have a few like questions and solutions so maybe having like this type of questions even if they're even we could use all one to create. Like a small data set out of this then we can use that to sort of like once the final the full final pipeline ready we can use this one to sort of probe the model into more complex thinking and more complex improvements because. Assuming this questions are smarter or way harder than the international map of limpia then this would be a good way to sort of have the model see what's hard or what would be complicated and what would be the process of more complicated questions because we never know maybe. The particular private set that they give us might be harder than the previous training training versions that we used. So this is the paper it's all on what's up and I think I already added in the project resources but yeah maybe let me see the performance. Oh yeah they did interview some mathematicians like Terence how if you know him he's a sealed medalist but yeah he did say it's like a hard it's really hard even for humans and he's one of the best he just said they're really hard so we can see here's the performance like yeah it's no any maybe they should have put like human. To see what's the comparison to like humans but it's okay but yeah I basically finished with the recording if you have any questions feel free to ask or if you have or if you want me to go through this again and. I'm going to start data side because that's the fasting the pipeline so what I'll do is I'll go through. I'll start the date we have their prior date by look for data I did find someone called or not called but hugging face for particular benchmark as well as this one so hard that's ready and cleaned and all of that then I'll miss laughing good to the next time if you want to start then it's fine but yeah I think that's the best. That's basically it.