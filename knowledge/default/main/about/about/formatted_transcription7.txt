Agent 0 version 0.7 is being released. So here are the updates. First of all, big thanks to Alessandro from the community who did a revamp of the UI.

 So now we have a chat interface that is mobile friendly responsive. It supports both light and dark theme and it's overall much nicer and smooth. Some more features of the new UI is the autoscroll switch automatically toggling based on the scroll position of the window. And all the framework messages are grouped under these utility messages which you can turn on and off. I almost forgot to mention that there is now a progress bar here saying waiting for input. So even if you have the utility messages off, you can see what exactly is the framework you're doing at any given time. But the biggest update in this version is the memory system. Now the memory is partly managed automatically by the framework and partly manually by the agent himself. So as you can see when you enable utility messages at the start of each model accession by the agent, memories are automatically loaded and injected into the system prompt. So when I ask the agent to download a thumbnail, some memory fragments from the past and also solutions from the past will be loaded into the memory based on the prompt. The agent can then use that knowledge and at the end of this session, he will memorize fragments containing important information and also successful solution if there were any. The memory of the agent is now divided into four areas. Main is the area where the agent will mostly operate on its own, saving memories there and from there, for example, if you tell the agent your name or if you give a API key for something, it will store it to the main area by default. Fragments and solutions are managed automatically by the framework. They are loaded and injected into the system prompt at the start of each model accession by the agent and they are stored and updated at the end of each model. Fragments are pieces of information found in the last conversation and solution are specifically for solution guides for next time. And as you can see here, two entries were memorized now where I told the agent what my name is and the framework automatically updated older information about my name with newer information based on similarity threshold between these two. And memories are also saved with proper metadata IDs, timestamps and areas and the agent can now use this metadata to filter memories when he searches for them. So if you for example only want memories from yesterday or from specific area, you can tell your agent what to search for. And I will get to the instrument section with the next tool. This is how it looks when you tell your agent to search for YouTube videos from yesterday. He will use the memory load tool, query YouTube video with the threshold limit of results and he can build his own filter specifying the start date and end date. The knowledge functionality and folder structure is now aligned with these memory sections. Under knowledge there are two subfolders default and custom. The difference is that custom is ignored by GitHub. So you can store your personal knowledge here and it will not be pushed to GitHub when you submit a request for example. And you can upload your files into these folders and they will be automatically imported into the memory when the memory database is initialized. Here are subfolders by the memory areas so anything you upload here will be imported into the main section everything under solutions will be imported into solutions. I put some example files here for example, a simple solution on how to get current time in time zone using Python. So when you ask your agent for current time, here is most likely to use this solution from his memory. And I imported some information from the framework itself like for example, the readme file from GitHub and the installation guide into the main area. So you can ask agents your questions about the framework and thanks to this automatic import into the memory, you will be able to answer questions about the framework itself. As you can imagine, this is most useful when you want to feed your agent with lots of information on your specific use case, on your environment, your personal data, etc. Another key feature of this version are instruments. Instruments are like tools for your agent. These tools can be used by the agent on his own, but these are not executed in the framework on your machine. These are executed inside the Docker container. And to create a custom instrument, you just need to create a markdown file under instruments where you define or you instruct your agent how to use it. In this case, I create an example YouTube download. I created a simple Linux shell script, but this can be a Python script. This can be JavaScript executed in Node.js. And here I just install the necessary libraries with one command and run YTDRP with another command using the argument from the command line. And here in the instructions, I just tell your agent that to download a YouTube video, he needs to CD into the desired location and run the instrument with bash and the path to this executable file. Now this path is the same in the framework folder as it is in the Docker container because this instrument folder is mapped to instruments folder inside the Linux. So the agent will know it's the same path to the file and he will be able to use it to download a YouTube video. So any specific tools, APIs you want to use and that the agent is not able to figure out on his own, you can create here under the instruments. You don't need to follow this template. You can create your own, you can create your own folder structure under instruments. The only thing you need to know is that every Markdown file under instruments will be imported into the memory automatically. Just like with the knowledge, this will be imported into the instruments area of the memory. And this whole folder is accessible by the agent in the container. So even the agent can create his own instruments if you tell him to just remember to create a Markdown file with instructions on how to call it. And this is how it looks when I tell the agent to download a YouTube video. If I enable these utility messages, we can see that the instrument is automatically injected into the prompt. And so the agent should follow along and call the instrument based on these instructions. And we can see that the video has been downloaded. To make these new features maintainable and organized, a new extensions framework has been developed. So under Python folder, there is extension subfolder. And here there are subfolders for various places inside the message group of the agent. So for example, message group prompt is called right here where the system message and the message history is being built. And all the files inside this folder are executed in their alphabetical order. So for example, here I prepare the main system prompt and the prompt for the tools. Here I can recall memories from the database and here I can recall solutions. And at the end of the monologue, I can memorize fragments, memorize solutions. In the future, most of the code in the framework will be in form of extensions or tools and helpers so that the main agent file is kept clean and simple. Once more change in prompts and files in general in this framework is that they can now include other files. So just by doing these double curly brackets, the keyword include and the path to another file. It will be included here. So now I could split the system prompt into sections and keep it organized. And it should be also easier to redefine the system prompt because if you want to use a custom prompt subfolder, you only create the files with the same names you want to replace. You don't have to copy the full default folder. You only copy or recreate files with the same names you want to change. Here I would like to thank Alessandro again for creating this DNA of prompts small and large. These prompts contain reflection. So if you want your agent to be able not only to use chain of thought but also reflection, you can specify the sub directory here. In the initialize.py file prompts subdom. Let's restart the framework and test it out. And let's give it a little. This is a little generated by chatGPT.

 So it probably should be no problem for chatGPT to solve. But here you can see that the agent is using the thoughts first, reflection of the thoughts second and then the revised thoughts and lastly the response tool to response. It says it's an echo and it also said the answer is an echo when I asked him in the chatGPT window. Again, it's the same model so it probably wasn't hard but it's a good thing to demonstrate the reflection capability. So as you can see just by changing a few English text files in the prompts directory, you can implement the chain of thought, reflection and many other AI concepts. So I would really like to encourage users to experiment with agent zero, especially now with the new memory system, with the instruments system. When all these things come together, I believe you can really push agent zero much closer to its potential. Don't forget to check out other videos on this channel containing the installation guide, the guide on how to run free and local models and updates from previous versions. If you want to join the community, you can do so on school or on our Discord server. All the links are in the description below the video. To wrap it up, let me say a big thanks to all the community members, especially for their contribution with pull requests and testing and ideas, but also for their patience and attitude. Thank you and see you next time.